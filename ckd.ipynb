{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnóstico de Doença Renal Crónica com Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorboard.plugins.projector as tf_projector\n",
    "import scipy.io.arff\n",
    "from time import time\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "Este tutorial cobre a utilização da biblioteca Tensorflow para o diagnóstico de Doença Renal Crónica com uma rede neuronal do tipo feed-forward. Este processo envolve os seguintes passos:\n",
    "\n",
    "- Pré-processamento dos dados (Scipy e Numpy)\n",
    "- Construção da rede (Tensorflow)\n",
    "- Treino e avaliação da rede (Tensorflow)\n",
    "- Visualização de métricas de treino (Tensorboard)\n",
    "- Visualização do dataset (Tensorboard)\n",
    "\n",
    "O dataset usado, assim como a descrição de cada atributo e classe está disponível em https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\n",
    "\n",
    "Tensorflow é uma biblioteca de computação numérica baseada em grafos de computação. Construimos um grafo adicionando-lhe nós que representam operações matemáticas. Estas operam sobre tensores, a estrutura de dados que dá o nome à biblioteca, que são a generalização de escalares, vectores, matrizes, etc. a qualquer número de dimensões. A vantagem da utilização deste grafo é que permite que toda a computação a ser realizada seja conhecida pelo ambiente de execução desde que este é iniciado. A utilização deste modelo traz várias vantagens: \n",
    "\n",
    "- Redução do overhead de comunicação entre o código Python e o Tensorflow, escrito em C++\n",
    "- Variedade de otimizações das operações que seria impossível se estas tivessem de ser executadas uma a uma\n",
    "- Cálculo de derivadas de expressões baseadas na regra da cadeia para funções compostas\n",
    "\n",
    "Este último ponto é essencial para o usa da biblioteca para a implementação de redes neuronais. Para qualquer expressão matemática diferenciável, a biblioteca é capaz de automaticamente calcular o gradiente em ordem a cada variável, eliminando a necessidade de implementar o algoritmo de backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = 3\n"
     ]
    }
   ],
   "source": [
    "# tf usado para aceder à biblioteca\n",
    "one = tf.constant(1)                            # tensor contem escalar constante constante\n",
    "\n",
    "# placeholder é um valor que será fornecido ao correr o grafo\n",
    "x = tf.placeholder(                             \n",
    "    dtype=\"int32\",                              # tipo do placeholder\n",
    "    shape=[],                                   # forma [] corresponde um escalar\n",
    "    name=\"x\"                                    # nome do tensor\n",
    ")\n",
    "\n",
    "# adiciona operação de soma entre x e one ao grafo\n",
    "# apenas toma um valor ao correr o grafo com run()\n",
    "c = tf.add(one, x)                              # equivalente a one + x\n",
    "\n",
    "# sessões encapsulam ambiente e o estado em que correm o grafo, with encarrega-se de iniciar e fechar a sessão\n",
    "with tf.Session() as sess:                     \n",
    "    # na sessão sess, calcular o valor de c, dando o valor 2 a x e atribuir o resultado à variável res\n",
    "    res = sess.run(c, feed_dict={x:2})\n",
    "    print(\"c = {}\".format(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A avaliação apenas é feita ao chamar a função run(), e apenas nos nós que são necessários para calcular os valores pedidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a  = 1\n",
      "ab = 3\n",
      "bc = 5\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.constant(4)\n",
    "\n",
    "ab = tf.add(a,b)\n",
    "bc = tf.add(b,c)\n",
    "cd = tf.add(c,d)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a_vl, ab_val, bc_val = sess.run([a, ab, bc]) # apenas são avaliadas as operações de que dependem as operações \n",
    "                                                 # passadas em run(), neste caso, cd não é avaliado. \n",
    "                                                 # run() retorna um valor por cada operação que lhe é passada\n",
    "    print(\"a  = {}\\nab = {}\\nbc = {}\".format(a_vl, ab_val, bc_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento de dados\n",
    "\n",
    "O primeiro passo para o treino consiste no pré-processamento dos dados. O dataset contém 400 individuos, cada um com 24 \n",
    "atributos e uma classificação binária, com ou sem doença. Alguns destes atributos são nominais e terão de ser convertidos \n",
    "para numéricos. Cada atributo nominal com n valores possíveis é substituído por n atributos binários, que indicam se o \n",
    "indíviduo tem ou não aquele valor no atributo original. Os atributos que já são binários não necessitam desta conversão \n",
    "mas esta é feita neste caso para simplificar o tratamento dos dados.\n",
    "\n",
    "Inicialmente é feita a conversão dos campos nominais para vários binários, depois são substituídos os valores desconhecidos\n",
    "e, finalmente, os dados são normalizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = \"chronic_kidney_disease.arff\"\n",
    "data, meta = scipy.io.arff.loadarff(filename)\n",
    "data_copy = np.copy(data)\n",
    "\n",
    "\"\"\"\n",
    "lb = skl.preprocessing.LabelBinarizer()\n",
    "for name, ty in meta.__dict__.get(\"_attributes\").items():\n",
    "    print(name, ty)\n",
    "    if ty[0] == \"nominal\":\n",
    "        lb.fit([st.encode(encoding='UTF-8') for st in ty[1]])\n",
    "        temp = lb.transform(data[name]).astype(np.float64)\n",
    "        print(temp)\n",
    "\"\"\"\n",
    "\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "from numpy.lib.recfunctions import drop_fields\n",
    "\n",
    "def pre_process(data, class_name):\n",
    "    # contagem do número de classes existentes para serem separados dos atributos posteriormente\n",
    "    num_classes = 0\n",
    "    # vai permitir substituir os valores dum nparray por floats\n",
    "    # se o valor for desconhecido, é substituído por NaN, se o valor\n",
    "    # for igual ao passado, é substituido por 1, se não, é substituído por 0\n",
    "    vec = lambda preset: np.vectorize(lambda v: np.nan if v == b'?' else 1. if v == preset else 0.)\n",
    "    for i, field in enumerate(data.dtype.names):\n",
    "        type_of_field = meta.types()[i]                       # tipo do campo pode ser nominal ou numérico\n",
    "        \n",
    "        if type_of_field == \"nominal\":                        # apenas serão tratados os campos nominais\n",
    "            \n",
    "            valid = data[field][data[field] != b'?']          # extrair os valores conhecidos\n",
    "            \n",
    "            unique = np.unique(valid).tolist()                # lista com os valores possíveis do atributo atual\n",
    "            \n",
    "            for val in unique:\n",
    "                # para cada valor val possível do atributo field, criamos um novo campo chamado field_val\n",
    "                # que terá o valor 1 para os indivíduos com o valor val no atributo field_val e 0 nos restantes\n",
    "                new_field = field + \"_\" + val.decode(\"utf-8\") \n",
    "                data = append_fields(\n",
    "                    data,\n",
    "                    new_field,\n",
    "                    vec(val)(data[field])\n",
    "                )\n",
    "            data = drop_fields(data, field) # eliminar campo original\n",
    "            \n",
    "    for i, field in enumerate(data.dtype.names):\n",
    "        if field[:len(class_name)] == class_name:             # não normalizar classes\n",
    "            num_classes += 1\n",
    "            continue\n",
    "        valid = data[field][~np.isnan(data[field])]           # extrair valores conhecidos\n",
    "        \n",
    "        std = valid.std()                                     \n",
    "        mean = valid.mean()\n",
    "        \n",
    "        data[field][np.isnan(data[field])] = mean             # subsituir valores desconhecidos pela média dos valores \n",
    "                                                              # conhecidos do campo atual\n",
    "            \n",
    "        data[field] = (data[field] - mean) / std              # normalizar valores, subtraindo a média e divindo pelo\n",
    "                                                              # desvio padrão do campo atual, resulta numa distribuição \n",
    "                                                              # com média 0 e desvio padrão unitário\n",
    "    return num_classes, data\n",
    "\n",
    "num_classes, data = pre_process(data=data, class_name=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extracção dos valores do structered array para ndarrays\n",
    "\n",
    "# converter o structured array para um array numpy de floats\n",
    "data_array = data.view(np.float64)\n",
    "\n",
    "# mudar a forma do array para ter número de linhas correspondente ao número de indíviduos\n",
    "data_array = data_array.reshape(data.shape[0], -1)\n",
    "\n",
    "# extrair apenas os atributos\n",
    "# para todas as linhas, extrair da primeira coluna até a última - num_classes coluna\n",
    "inputs = data_array[:, :-num_classes]\n",
    "\n",
    "# extrair as classes\n",
    "# para todas as linhas, extrair as últimas num_classes colunas\n",
    "labels = data_array[:, -num_classes:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção da rede\n",
    "\n",
    "De modo a construir a rede, para cada camada da rede, adicionando as seguintes operações:\n",
    "- multiplicação matricial entre output da camada anterior e pesos da camada actual\n",
    "- soma dos biases da camada atual\n",
    "- se camada não for final, função de ativação\n",
    "\n",
    "$$o^{i} = o^{i-1} . W^{i} + b^{i} $$\n",
    "\n",
    "A utilização de keyword arguments é opcional e é usada aqui para explicitar os argumentos usados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construção de uma camada da rede com  num_units neurónios\n",
    "def make_layer(inputs, num_units, name, final = True):\n",
    "    \n",
    "    with tf.name_scope(name):                          # name scope permite agrupar um conjunto de operações\n",
    "        \n",
    "        # variáveis são usadas para os valores que vão sendo alterados, têm trainable=True por defeito\n",
    "        # e são por isso atualizadas automaticamente de modo a diminuir o erro no processo de treino\n",
    "        weights = tf.Variable(                         \n",
    "            initial_value= tf.truncated_normal(        # pesos inicializados aleatóriamente com uma distribuição normal\n",
    "                [inputs.shape[1].value, num_units],    # matriz de tamanho (nr colunas de input para a camada) * (nº de neurónios)\n",
    "                dtype=tf.float64                       # tipo dos valores gerados\n",
    "            ),                     \n",
    "            name=name + \"_weights\"                     # nome permite identificar fácilmente a variável no Tensorboard\n",
    "        )\n",
    "        \n",
    "        biases = tf.Variable(\n",
    "            tf.zeros(                                  # inicializar a 0s\n",
    "                [num_units],                           # vetor com um valor por neurónio\n",
    "                dtype=tf.float64\n",
    "            ),    \n",
    "            name=name + \"_biases\"\n",
    "        )\n",
    "        \n",
    "        #permite a visualização da distribuição das variáveis ao longo do treino\n",
    "        tf.summary.histogram(\n",
    "            name=\"weights\", \n",
    "            values=weights\n",
    "        ) \n",
    "        tf.summary.histogram(\n",
    "            name=\"biases\",\n",
    "            values=biases\n",
    "        )\n",
    "        \n",
    "        # multiplicação de matrizes\n",
    "        ret = tf.matmul(inputs, weights) + biases      # soma com tensores equivalente a tf.add()\n",
    "        \n",
    "        if not final:\n",
    "            # função de ativação ReLU https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "            # não é usada na camada final\n",
    "            ret = tf.nn.relu(features=ret)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def make_net(inputs, hidden_units, classifications):\n",
    "    in_val = inputs\n",
    "    for i, num_units in enumerate(hidden_units):\n",
    "        # criar uma camada intermédia para cada elemento de hidden_units\n",
    "        in_val = make_layer(\n",
    "            in_val, \n",
    "            num_units, \n",
    "            \"layer_{}\".format(i),\n",
    "            False\n",
    "        )\n",
    "    # camada final\n",
    "    out_val = make_layer(\n",
    "        in_val, \n",
    "        classifications.shape[1].value, \n",
    "        \"final_layer\",  \n",
    "        True\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope(\"post\"): # cálculo de probabilidades, custo, \n",
    "        # softmax transforma outputs da camada final nas probabilidades para cada classe\n",
    "        # https://en.wikipedia.org/wiki/Softmax_function\n",
    "        probabilities = tf.nn.softmax(\n",
    "            logits=out_val, \n",
    "            name=\"soft_max\"\n",
    "        )\n",
    "        \n",
    "        cost = tf.reduce_mean(                           # custo é média da cross entropy para todas as classes\n",
    "            tf.nn.softmax_cross_entropy_with_logits(     # http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "                labels=classifications,                  # classificações vindas dos dataset\n",
    "                logits=out_val,                          # valores de saída da camada final\n",
    "                name=\"cross\"\n",
    "            ), \n",
    "            name=\"cost\"\n",
    "        )\n",
    "        \n",
    "        # optimizer permite treinar os pesos e biases da rede\n",
    "        # ao avaliar esta operação, o gradiente é calculado automáticamente\n",
    "        # e propagado pela rede, atualizando o valor dos pesos e biases\n",
    "        # existem vários otimizadores definidos em tf.train\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=0.01,                          # usamos uma learning rate constante, mas pode variar com o step\n",
    "            name=\"opt\"\n",
    "        ).minimize(                                      # indicar que o custo deve ser minimizado\n",
    "            cost, \n",
    "            name=\"minimizer\"\n",
    "        )           \n",
    "        \n",
    "        # a rede escolhe a classificação com um maior valor de output da camada final\n",
    "        # classificação é o índice do maior valor\n",
    "        predictions = tf.argmax(\n",
    "            input=out_val,\n",
    "            dimension=1,\n",
    "            name=\"predictions\"\n",
    "        )\n",
    "        \n",
    "        # são correctas as previsões iguais às labels do dataset\n",
    "        correct_predictions = tf.equal(\n",
    "            x=predictions,\n",
    "            y=tf.argmax(\n",
    "                input=classifications,                   # vindos do data set\n",
    "                dimension=1\n",
    "            ), \n",
    "            name=\"correct\"\n",
    "        )\n",
    "        \n",
    "        # precisão é a fracção de previsões feitas corretamente\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(                                     # conversão de booleanos para floats para calcular a média\n",
    "                x=correct_predictions, \n",
    "                dtype=tf.float64\n",
    "            ), \n",
    "            name=\"acc\"\n",
    "        )\n",
    "        \n",
    "        # permite a visualização das métricas de custo e precisão ao longo do treino\n",
    "        tf.summary.scalar(\n",
    "            name=\"acc\", \n",
    "            tensor=accuracy\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"cost\", \n",
    "            tensor=cost\n",
    "        )\n",
    "    \n",
    "    return probabilities, optimizer, predictions, correct_predictions, accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treino e avaliação da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99: Test Accuracy= 0.5850000000000001\n",
      "Step 199: Test Accuracy= 0.61\n",
      "Step 299: Test Accuracy= 0.62\n",
      "Step 399: Test Accuracy= 0.635\n",
      "Step 499: Test Accuracy= 0.785\n",
      "Step 599: Test Accuracy= 0.8350000000000001\n",
      "Step 699: Test Accuracy= 0.8350000000000001\n",
      "Step 799: Test Accuracy= 0.8500000000000001\n",
      "Step 899: Test Accuracy= 0.8450000000000001\n",
      "Step 999: Test Accuracy= 0.855\n"
     ]
    }
   ],
   "source": [
    "# fazer reset ao grafo, para eliminar nós já introduzidos\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# vai conter os atributos dos invíduos\n",
    "X = tf.placeholder(                                  \n",
    "    dtype=\"float64\",                                     # tipo de dados do tensor\n",
    "    \n",
    "    shape=[None, inputs.shape[1]],                       # o tensor passado será uma matriz com uma linha por indivíduo\n",
    "                                                         # e uma coluna por atributo. O número de indíviduos pode variar\n",
    "                                                         # e por isso é usado o valor None para este\n",
    "    name=\"X\"\n",
    ")\n",
    "\n",
    "# vai conter as classificações vindas do dataset\n",
    "y = tf.placeholder(\n",
    "    dtype=\"float64\", \n",
    "    shape=[None, labels.shape[1]], \n",
    "    name=\"y\"\n",
    ")\n",
    "\n",
    "keep_prob = tf.placeholder(\n",
    "    dtype=tf.float64, \n",
    "    name=\"keep_prob\"\n",
    ")\n",
    "\n",
    "# chamada da função de construção da rede definida acima\n",
    "probabilities, optimizer, predictions, correct_predictions, accuracy = make_net(X, [4,4], y)\n",
    "\n",
    "# operação de inicialização das variávies (pesos e biases)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# número máximo de iterações\n",
    "num_epochs = 1000\n",
    "\n",
    "# percentagem dos indíviduos a usar no conjunto de teste\n",
    "# é usado para verificar o poder de previsão da rede em indíviduos\n",
    "# que não foram usados no treino\n",
    "test_size = 0.5\n",
    "\n",
    "# percentagem do indíviduos do conjunto de treino a usar no conjunto de validação\n",
    "validation_size = 0.2\n",
    "\n",
    "# operação que junta todos as operações de summary\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "LOG_DIR = os.path.join('.', 'logs')\n",
    "\n",
    "def make_file_writer(dataset, curr_time, session):\n",
    "    return tf.summary.FileWriter(\n",
    "        os.path.join(LOG_DIR, \"{}\", \"{}\").format(dataset, curr_time),\n",
    "        session.graph\n",
    "    )\n",
    "\n",
    "# baralha os indivíduos e separa-os em conjunto de treino e teste\n",
    "# stratified significa que a divisão é feita mantendo a mesma proporção\n",
    "# de indivíduos de cada classe em cada \n",
    "kf = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "    n_splits=1, \n",
    "    test_size=test_size\n",
    ")\n",
    "\n",
    "\n",
    "# iterar sobre as diferentes formas de separar os dados em treino e teste\n",
    "# como n_splits=1 acima, apenas é retornada uma\n",
    "# os dados são separados uma única vez em treino e teste\n",
    "# em cada iteração, os dados de treino vão ser separados em treino e validação\n",
    "for train_temp_i, test_i in kf.split(inputs, labels):   \n",
    "    \n",
    "    # split retorna os indices a usar, aqui obtemos os dados que os indices referem\n",
    "    train_temp_inputs = inputs[train_temp_i]\n",
    "    train_temp_labels = labels[train_temp_i]\n",
    "    test_inputs       = inputs[test_i]\n",
    "    test_labels       = labels[test_i]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # usamos o tempo atual para por os dados de cada run num directório diferente\n",
    "        curr_time = time()\n",
    "        \n",
    "        # para escrever estatísticas em ficheiros ao longo do processo de treino\n",
    "        # escrevemos as estatísticas de treino, validação e teste em ficheiros diferentes para os comparar\n",
    "        train_writer = make_file_writer(\"train\", curr_time, sess)\n",
    "        valid_writer = make_file_writer(\"valid\", curr_time, sess)\n",
    "        test_writer  = make_file_writer(\"test\" , curr_time, sess)\n",
    "        \n",
    "        sess.run(init)\n",
    "        for i in range(num_epochs):\n",
    "            # em cada iteração, separar os dados de treino nos conjuntos de validação e de treino\n",
    "            # mantendo proporção de indíviduos de cada classe\n",
    "            kf2 = sklearn.model_selection.StratifiedShuffleSplit(\n",
    "                n_splits=1, \n",
    "                test_size=validation_size\n",
    "            )\n",
    "            \n",
    "            for train_i, validation_i in kf2.split(train_temp_inputs, train_temp_labels):\n",
    "                # aceder aos indices retornados\n",
    "                train_inputs      = train_temp_inputs[train_i]  \n",
    "                train_labels      = train_temp_labels[train_i]\n",
    "                validation_inputs = train_temp_inputs[validation_i]\n",
    "                validation_labels = train_temp_labels[validation_i]\n",
    "                \n",
    "                # treino da rede\n",
    "                _, stats = sess.run(\n",
    "                    [optimizer, merged],\n",
    "                    feed_dict={X: train_inputs, y: train_labels}\n",
    "                )\n",
    "                \n",
    "                # escrever as estatísticas de treino para o passo i\n",
    "                train_writer.add_summary(\n",
    "                    summary=stats,\n",
    "                    global_step=i\n",
    "                )        \n",
    "                \n",
    "                # validação (como não é passada a operação de minimização, a rede não é treinada)\n",
    "                acc, stats = sess.run(\n",
    "                    [accuracy, merged], \n",
    "                    feed_dict={X: validation_inputs, y: validation_labels}\n",
    "                )\n",
    "                \n",
    "                # escrever as estatísticas de validação para o passo i\n",
    "                valid_writer.add_summary(\n",
    "                    summary=stats,\n",
    "                    global_step=i\n",
    "                )        \n",
    "            \n",
    "            # periodicamente verificar a performance da rede no conjunto de treino\n",
    "            # a rede nunca é treinada com este conjunto\n",
    "            if (i+1) % 100 == 0:\n",
    "                # ao não passar a operação de minimização, a rede não é treinada\n",
    "                acc, stats = sess.run(\n",
    "                    [accuracy, merged], \n",
    "                    feed_dict={X: test_inputs, y: test_labels}\n",
    "                )\n",
    "                # escrever as estatísticas de teste para o passo i\n",
    "                test_writer.add_summary(\n",
    "                    summary=stats, \n",
    "                    global_step=i\n",
    "                )         \n",
    "                print(\"Step {}: Test Accuracy= {}\".format(i, acc))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização do treino\n",
    "\n",
    "Corremos o Tensorboard com o seguinte commando, no directório onde foi corrido o script:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./logs --reload_interval=1\n",
    "```\n",
    "\n",
    "O primeiro argumento indica o directório onde foram escritas os sumários e o segundo o período de tempo de atualização dos dados visualizados.\n",
    "\n",
    "Abrindo o browser na porta indicada (que por defeito é a 6006) vemos um ecrã inicial:\n",
    "\n",
    "<img src=\"images/tensorboard_home.JPG\" />\n",
    "\n",
    "_post_ é o único namescope com operações summary do tipo escalar neste exemplo. Clicando na tab, podemos ver os gráficos produzidos\n",
    "\n",
    "<img src=\"images/tensorboard_plots.JPG\" />\n",
    "\n",
    "Na secção da esquerda encontram-se os ficheiros que foram escritos e permite escolher quais são visualizados. Neste caso é criado um para treino, validação e teste para cada *run* com o *timestamp* em que foram corridos como nome.\n",
    "\n",
    "As tabs *Distributions* e *Histograms* permitem visualizar as distribuições das variáveis (pesos e biases de cada camada) em função do step.\n",
    "\n",
    "<img src=\"images/tensorboard_distributions.JPG\" />\n",
    "<img src=\"images/tensorboard_histograms.JPG\" />\n",
    "\n",
    "Finalmente, a tab *Graphs* permite visualizar o grafo de computação construído:\n",
    "\n",
    "<img src=\"images/tensorboard_graphs.JPG\" />\n",
    "\n",
    "Os quadrados visíveis inicialmente representam os namescopes utilizados. Clicando sobre o \"+\", é possível ver as operações que os compõem.\n",
    "\n",
    "<img src=\"images/tensorboard_layer.JPG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização do dataset\n",
    "\n",
    "Podemos usar o *Embedding Visualizer* para projetar dados com muitas dimensões para 2 ou 3 de modo a visualizá-los.\n",
    "Neste exemplo visualizamos o dataset de pacientes baseado nos seus atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "Failed to rename: .\\logs\\embeddings\\model.ckpt.index.tempstate1701704182032771794 to: .\\logs\\embeddings\\model.ckpt.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, attributes/_1)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-1becfb23d650>\", line 45, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1040, in __init__\n    self.build()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1070, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 673, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 271, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 214, in save_op\n    tensors)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 779, in save_v2\n    tensors=tensors, name=name)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nUnknownError (see above for traceback): Failed to rename: .\\logs\\embeddings\\model.ckpt.index.tempstate1701704182032771794 to: .\\logs\\embeddings\\model.ckpt.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, attributes/_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    467\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to rename: .\\logs\\embeddings\\model.ckpt.index.tempstate1701704182032771794 to: .\\logs\\embeddings\\model.ckpt.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, attributes/_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1becfb23d650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMBED_DIR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model.ckpt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[0;32m   1361\u001b[0m       model_checkpoint_path = sess.run(\n\u001b[0;32m   1362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m           {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1364\u001b[0m       \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1365\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mwrite_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mc:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1035\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: Failed to rename: .\\logs\\embeddings\\model.ckpt.index.tempstate1701704182032771794 to: .\\logs\\embeddings\\model.ckpt.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, attributes/_1)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-1becfb23d650>\", line 45, in <module>\n    saver = tf.train.Saver()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1040, in __init__\n    self.build()\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1070, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 673, in build\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 271, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 214, in save_op\n    tensors)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 779, in save_v2\n    tensors=tensors, name=name)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\ricardo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nUnknownError (see above for traceback): Failed to rename: .\\logs\\embeddings\\model.ckpt.index.tempstate1701704182032771794 to: .\\logs\\embeddings\\model.ckpt.index : Access is denied.\r\n; Input/output error\n\t [[Node: save/SaveV2 = SaveV2[dtypes=[DT_DOUBLE], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, attributes/_1)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "EMBED_DIR = os.path.join(LOG_DIR, 'embeddings')\n",
    "\n",
    "path_for_metadata =  os.path.join(EMBED_DIR, 'metadata.tsv')\n",
    "\n",
    "# escrever metadados num ficheiro .tsv (tab separated values)\n",
    "# para ponto de dados, escrevemos os atributos com que pretendemos que eles sejam etiquetados\n",
    "with open(path_for_metadata,'w') as f:\n",
    "    sep = \"\\t\"\n",
    "    f.write(\"Index\")\n",
    "    for field in data.dtype.names:\n",
    "        if field[:5] == \"class\":\n",
    "            continue\n",
    "        f.write(sep + field)\n",
    "    f.write(sep + \"Labels\\n\")\n",
    "    for index in range(len(inputs)):\n",
    "        f.write(\"{}\".format(index))\n",
    "        for field_i, field in enumerate(data.dtype.names):\n",
    "            if field[:5] == \"class\":\n",
    "                continue\n",
    "            # Tensorboard ainda não permite definir as cores dos pontos\n",
    "            # usamos aqui um valor aleatório para assegurar que é usado um gradiente de cores\n",
    "            f.write(\"{}{}\".format(sep, data[field][index] + random.uniform(0, 0.0001)))\n",
    "            \n",
    "        f.write(\"{}{}\\n\".format(sep, 'ckd' if labels[index][0] == 1 else 'notckd'))\n",
    "\n",
    "# tensor com os valores que irão afetar a projecção dos pontos no espaço\n",
    "# neste caso queremos mostrá-los com base nos seus atributos\n",
    "embedding_var = tf.Variable(inputs, name=\"attributes\")\n",
    "summary_writer = tf.summary.FileWriter(EMBED_DIR)\n",
    "\n",
    "config = tf_projector.ProjectorConfig()\n",
    "\n",
    "# podem ser adicionados mais que um tensor de embeddings\n",
    "embedding = config.embeddings.add()\n",
    "# nome identifica o tensor criado acima\n",
    "embedding.tensor_name = embedding_var.name\n",
    "# indicamos o caminho para o ficheiro onde se encontram os metadados\n",
    "embedding.metadata_path = path_for_metadata\n",
    "\n",
    "tf_projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(EMBED_DIR, \"model.ckpt\"))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando o TensorBoard com o comando:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=./logs/embeddings --reload_interval=1\n",
    "```\n",
    "\n",
    "E clicando na tab *Embeddings*:\n",
    "\n",
    "<img src=\"images/tensorboard_embedding_home.JPG\"/>\n",
    "\n",
    "Por defeito, os dados são mostrados com o algoritmo PCA, que escolhe os três eixos que maximizam a variância entre os pontos.\n",
    "\n",
    "Clicando no dropdown \"Color by\" podemos seleccionar os atributos com que os pontos serão coloridos. Seleccionando \"Label\", os pontos de classes diferentes são coloridos de forma diferente.\n",
    "\n",
    "<img src=\"images/tensorboard_embedding.JPG\"/>\n",
    "\n",
    "Os pontos podem ser seleccionados, mostrando o seu índice assim como outros pontos vizinhos. \n",
    "\n",
    "<img src=\"images/tensorboard_selection.JPG\"/>\n",
    "\n",
    "Conseguimos ver imediatamente uma distinção entre indivíduos com e sem a doença.\n",
    "\n",
    "Podem também ser visualizados com o algoritmo t-SNE, com parâmetros modificáveis. Este algoritmo tenta agrupar espacialmente pontos com características semelhantes.\n",
    "\n",
    "<img src=\"images/tensorboard_tsne.JPG\"/>\n",
    "\n",
    "Conseguimos ver que existem dois clusters principais de indivíduos sem a doença, enquanto que a maioria dos que têm a doença se encontram noutro. Isto explica a facilidade com que a rede neuronal consegue diagnosticar os pacientes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
