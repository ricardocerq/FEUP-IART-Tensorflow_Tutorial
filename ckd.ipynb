{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.tensorboard.plugins.projector as tf_projector\n",
    "import scipy.io.arff\n",
    "from numpy.lib.recfunctions import append_fields\n",
    "from numpy.lib.recfunctions import drop_fields\n",
    "from time import gmtime, strftime, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as skl\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Este tutorial cobre a utilização da biblioteca para o diagnóstico de Doença Renal Crónica com uma rede neuronal do tipo\n",
    "feed-forward, visualização de métricas de treino e finalmente visualização dos dados usando Tensorboard. \n",
    "O dataset usado, assim como a descrição de cada atributo e classe está disponível em \n",
    "https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease\n",
    "\n",
    "Tensorflow é uma biblioteca de computação numérica baseada em grafos de computação. Construimos o grafo adicionando-lhe nós\n",
    "que representam operações matemáticas. Estas operam sobre tensores, que são a generalização de escalares, vectores, matrizes,\n",
    "etc a qualquer número de dimensões e constituem o tipo de dados principal da biblioteca. No entanto estas apenas correm\n",
    "ao chamar a função run - a avaliação é lazy.\n",
    "\"\"\"\n",
    "\n",
    "one = tf.constant(1)                            # tf usado para aceder à library tensor constante\n",
    "\n",
    "x = tf.placeholder(                             # placeholder é um valor que será fornecido ao correr o grafo\n",
    "    dtype=\"int32\",                              # tipo do placeholder\n",
    "    shape=[],                                   # forma [] representa um escalar\n",
    "    name=\"x\"                                    # nome do tensor\n",
    ") \n",
    "c = tf.add(one, x)                              # retorna tensor com a soma de one e x, que ainda não tem valor\n",
    "\n",
    "with tf.Session() as sess:                      # sessões encapsulam ambiente em que correm as operações\n",
    "    res = sess.run(c, feed_dict={x:2})          # calcular o valor de c, substituindo x pelo valor 2, na sessão sess\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3 5\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(1)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.constant(4)\n",
    "\n",
    "ab = tf.add(a,b)\n",
    "bc = tf.add(b,c)\n",
    "cd = tf.add(c,d)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    a_vl, ab_val, bc_val = sess.run([a, ab, bc]) # apenas são avaliadas as operações de que dependem as operações \n",
    "                                                 # passadas em run(), neste caso, cd não é avaliado. \n",
    "                                                 # run() retorna um valor por cada operação que lhe é passada\n",
    "    print(a_vl, ab_val, bc_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "O primeiro passo consiste no pré-processamento dos dados. O dataset contém 400 individuos, cada um com 24 atributos e uma \n",
    "classificação binária, com ou sem doença. Alguns destes atributos são nominais e terão de ser convertidos para numéricos.\n",
    "Cada atributo nominal com n valores possíveis é substituído por n atributos binários, que indicam se o indíviduo tem ou não\n",
    "aquele valor no atributo original. Os atributos binários não necessitam desta conversão mas esta é feita neste caso para \n",
    "simplificar o tratamento dos dados.\n",
    "\n",
    "Inicialmente é feita a conversão dos campos nominais para vários binários, depois são substituídos os valores desconhecidos\n",
    "e finalmente, os dados são normalizados\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "filename = \"chronic_kidney_disease.arff\"\n",
    "data, meta = scipy.io.arff.loadarff(filename)\n",
    "data_copy = np.copy(data)\n",
    "\n",
    "\"\"\"\n",
    "lb = skl.preprocessing.LabelBinarizer()\n",
    "for name, ty in meta.__dict__.get(\"_attributes\").items():\n",
    "    print(name, ty)\n",
    "    if ty[0] == \"nominal\":\n",
    "        lb.fit([st.encode(encoding='UTF-8') for st in ty[1]])\n",
    "        temp = lb.transform(data[name]).astype(np.float64)\n",
    "        print(temp)\n",
    "\"\"\"\n",
    "\n",
    "def pre_process(data, class_name):\n",
    "    # contagem do número de classes existentes para serem separados dos atributos posteriormente\n",
    "    num_classes = 0\n",
    "    # vai permitir substituir os valores dum nparray por floats\n",
    "    # se o valor for desconhecido, é substituído por NaN, se o valor\n",
    "    # for igual ao passado, é substituido por 1, se não, é substituído por 0\n",
    "    vec = lambda preset: np.vectorize(lambda v: np.nan if v == b'?' else 1. if v == preset else 0.)\n",
    "    for i, field in enumerate(data.dtype.names):\n",
    "        type_of_field = meta.types()[i]                       # tipo do campo pode ser nominal ou numérico\n",
    "        \n",
    "        if type_of_field == \"nominal\":                        # apenas serão tratados os campos nominais\n",
    "            \n",
    "            valid = data[field][data[field] != b'?']          # extrair os valores conhecidos\n",
    "            \n",
    "            unique = np.unique(valid).tolist()                # lista com os valores possíveis do atributo atual\n",
    "            \n",
    "            for val in unique:\n",
    "                # para cada valor val possível do atributo field, criamos um novo campo chamado field_val\n",
    "                # que terá o valor 1 para os indivíduos com o valor val no atributo field e 0 nos restantes\n",
    "                new_field = field + \"_\" + val.decode(\"utf-8\") \n",
    "                data = append_fields(\n",
    "                    data,\n",
    "                    new_field,\n",
    "                    vec(val)(data[field])\n",
    "                )\n",
    "            data = drop_fields(data, field)                   # eliminar campo original\n",
    "            \n",
    "    for i, field in enumerate(data.dtype.names):\n",
    "        if field[:len(class_name)] == class_name:             # não normalizar classes\n",
    "            num_classes += 1\n",
    "            continue\n",
    "        valid = data[field][~np.isnan(data[field])]           # extrair valores conhecidos\n",
    "        \n",
    "        std = valid.std()                                     \n",
    "        mean = valid.mean()\n",
    "        \n",
    "        data[field][np.isnan(data[field])] = mean             # subsituir valores desconhecidos pela média dos valores \n",
    "                                                              # conhecidos do campo atual\n",
    "            \n",
    "        data[field] = (data[field] - mean) / std              # normalizar valores, subtraindo a média e divindo pelo\n",
    "                                                              # desvio padrão do campo atual, resulta numa distribuição \n",
    "                                                              # com média 0 e desvio padrão unitário\n",
    "    return num_classes, data\n",
    "\n",
    "num_classes, data = pre_process(data=data, class_name=\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Construção da rede\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# construção de uma camada da rede com  num_units neurónios\n",
    "# a utilização de keyword arguments é opcional e é usada aqui para explicitar os argumentos usados\n",
    "def make_layer(inputs, num_units, name, final = True):\n",
    "    with tf.name_scope(name):                          # name scope permite agrupar as operações da mesma camada\n",
    "        weights = tf.Variable(                         # variáveis têm trainable=True por defeito;\n",
    "            initial_value= tf.truncated_normal(        # pesos inicializados aleatóriamente com uma distribuição normal\n",
    "                [inputs.shape[1].value, num_units],    # inicialização duma matriz de tamanho nr colunas de input * nº de neurónios\n",
    "                dtype=tf.float64                       # tipo dos valores gerados\n",
    "            ),                     \n",
    "            name=name + \"_weights\"                     # nome permite identificar fácilmente a variável no Tensorboard\n",
    "        )\n",
    "        biases = tf.Variable(\n",
    "            tf.zeros(                                  # inicializar a 0s\n",
    "                [num_units],                           # um valor por neurónio\n",
    "                dtype=tf.float64\n",
    "            ),    \n",
    "            name=name + \"_biases\"\n",
    "        )\n",
    "        #permite a visualização da distribuição das variáveis ao longo do treino\n",
    "        tf.summary.histogram(\n",
    "            name=\"weights\", \n",
    "            values=weights\n",
    "        ) \n",
    "        tf.summary.histogram(\n",
    "            name=\"biases\",\n",
    "            values=biases\n",
    "        )\n",
    "        \n",
    "        # multiplicação de matrizes\n",
    "        ret = tf.matmul(inputs, weights) + biases      # soma com tensores equivalente a tf.add()\n",
    "        \n",
    "        if not final:\n",
    "            # função de ativação ReLU https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "            # não é usada na camada final\n",
    "            ret = tf.nn.relu(features=ret)\n",
    "            \n",
    "    return ret\n",
    "\n",
    "def make_net(inputs, hidden_units, classifications):\n",
    "    in_val = inputs\n",
    "    for i, num_units in enumerate(hidden_units):\n",
    "        # criar uma camada intermédia para cada elemento de hidden_units\n",
    "        in_val = make_layer(\n",
    "            in_val, \n",
    "            num_units, \n",
    "            \"layer_{}\".format(i),\n",
    "            False\n",
    "        )\n",
    "    # camada final\n",
    "    out_val = make_layer(\n",
    "        in_val, \n",
    "        classifications.shape[1].value, \n",
    "        \"final_layer\",  \n",
    "        True\n",
    "    )\n",
    "    \n",
    "    with tf.name_scope(\"post\"): # cálculo de probabilidades, custo, \n",
    "        # softmax transforma outputs da camada final nas probabilidades para cada classe\n",
    "        # https://en.wikipedia.org/wiki/Softmax_function\n",
    "        probabilities = tf.nn.softmax(\n",
    "            logits=out_val, \n",
    "            name=\"soft_max\"\n",
    "        )\n",
    "        \n",
    "        cost = tf.reduce_mean(                           # custo é média da cross entropy para todas as classes\n",
    "            tf.nn.softmax_cross_entropy_with_logits(     # http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "                labels=classifications,                  # classificações vindas dos dataset\n",
    "                logits=out_val,                          # valores de saída da camada final\n",
    "                name=\"cross\"\n",
    "            ), \n",
    "            name=\"cost\"\n",
    "        )\n",
    "        \n",
    "        # optimizer permite treinar os pesos e biases da rede\n",
    "        # ao avaliar esta operação, o gradiente é calculado automáticamente\n",
    "        # e propagado pela rede, atualizando o valor dos pesos e biases\n",
    "        # existem vários otimizadores definidos em tf.train\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=0.01,                          # usamos uma learning rate constante, mas pode variar com o step\n",
    "            name=\"opt\"\n",
    "        ).minimize(                                      # indicar que o custo deve ser minimizado\n",
    "            cost, \n",
    "            name=\"minimizer\"\n",
    "        )           \n",
    "        \n",
    "        # a rede escolhe a classificação com um maior valor de output da camada final\n",
    "        # classificação é o índice do maior valor\n",
    "        predictions = tf.argmax(\n",
    "            input=out_val,\n",
    "            dimension=1,\n",
    "            name=\"predictions\"\n",
    "        )\n",
    "        \n",
    "        # são correctas as previsões iguais às labels do dataset\n",
    "        correct_predictions = tf.equal(\n",
    "            x=predictions,\n",
    "            y=tf.argmax(\n",
    "                input=classifications,                   # vindos do data set\n",
    "                dimension=1\n",
    "            ), \n",
    "            name=\"correct\"\n",
    "        )\n",
    "        \n",
    "        # precisão é a fracção de previsões feitas corretamente\n",
    "        # idealmente deve ser calculada para cada classe\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(                                     # conversão de booleanos para floats para calcular a média\n",
    "                x=correct_predictions, \n",
    "                dtype=tf.float64\n",
    "            ), \n",
    "            name=\"acc\"\n",
    "        )\n",
    "        \n",
    "        # permite a visualização das métricas de custo e precisão ao longo do treino\n",
    "        tf.summary.scalar(\n",
    "            name=\"acc\", \n",
    "            tensor=accuracy\n",
    "        )\n",
    "        tf.summary.scalar(\n",
    "            name=\"cost\", \n",
    "            tensor=cost\n",
    "        )\n",
    "    \n",
    "    return probabilities, optimizer, predictions, correct_predictions, accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converter o structured array para um array numpy de floats\n",
    "data_array = data.view(np.float64)\n",
    "\n",
    "# mudar a forma do array para ter número de linhas correspondente ao número de indíviduos\n",
    "data_array = data_array.reshape(data.shape[0], -1)\n",
    "\n",
    "# extrair apenas os atributos\n",
    "# para todas as linhas, extrair da primeira coluna até a última - num_classes coluna\n",
    "inputs = data_array[:, :-num_classes]\n",
    "\n",
    "# extrair as classes\n",
    "# para todas as linhas, extrair as últimas num_classes colunas\n",
    "labels = data_array[:, -num_classes:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.305 0\n",
      "Test Accuracy:  0.73 100\n",
      "Test Accuracy:  0.785 200\n",
      "Test Accuracy:  0.815 300\n",
      "Test Accuracy:  0.85 400\n",
      "Test Accuracy:  0.86 500\n",
      "Test Accuracy:  0.875 600\n",
      "Test Accuracy:  0.89 700\n",
      "Test Accuracy:  0.895 800\n",
      "Test Accuracy:  0.905 900\n"
     ]
    }
   ],
   "source": [
    "# fazer reset ao grafo, para eliminar nós já introduzidos\n",
    "tf.reset_default_graph()\n",
    "                                                         #\n",
    "# vai conter os atributos dos invíduos\n",
    "X = tf.placeholder(                                  \n",
    "    dtype=\"float64\",                                     # tipo de dados do tensor\n",
    "    \n",
    "    shape=[None, inputs.shape[1]],                       # o tensor passado será uma matriz com uma linha por indivíduo\n",
    "                                                         # e uma coluna por atributo. O número de indíviduos pode variar\n",
    "                                                         # e por isso é usado o valor None para este\n",
    "    name=\"X\"\n",
    ")\n",
    "\n",
    "# vai conter as classificações vindas do dataset\n",
    "y = tf.placeholder(\n",
    "    dtype=\"float64\", \n",
    "    shape=[None, labels.shape[1]], \n",
    "    name=\"y\"\n",
    ")\n",
    "\n",
    "keep_prob = tf.placeholder(\n",
    "    dtype=tf.float64, \n",
    "    name=\"keep_prob\"\n",
    ")\n",
    "\n",
    "# chamada da função de construção da rede definida acima\n",
    "probabilities, optimizer, predictions, correct_predictions, accuracy = make_net(X, [], y)\n",
    "\n",
    "# operação de inicialização das variávies (pesos e biases)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# número máximo de iterações\n",
    "num_epochs = 1000\n",
    "error = 0.001\n",
    "\n",
    "# percentagem dos indíviduos a usar no conjunto de teste\n",
    "# é usado para verificar o poder de previsão da rede em indíviduos\n",
    "# que não foram usados no treino\n",
    "test_size = 0.5\n",
    "\n",
    "# percentagem do indíviduos do conjunto de treino a usar no conjunto de validação\n",
    "validation_size = 0.2\n",
    "\n",
    "# operação que junta todos as operações de summary\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "random.seed(42)\n",
    "seeds = random.sample(range(100000), num_epochs)\n",
    "\n",
    "LOG_DIR = os.path.join('.', 'logs')\n",
    "\n",
    "def make_file_writer(dataset, curr_time, session):\n",
    "    return tf.summary.FileWriter(\n",
    "        os.path.join(LOG_DIR, \"{}\", \"{}\").format(dataset, curr_time),\n",
    "        session.graph\n",
    "    )\n",
    "\n",
    "# baralha os indivíduos e separa-os em conjunto de treino e teste\n",
    "# stratified significa que a divisão é feita mantendo a mesma proporção\n",
    "# de indivíduos de cada classe em cada \n",
    "kf = skl.model_selection.StratifiedShuffleSplit(\n",
    "    n_splits=1, \n",
    "    test_size=test_size, \n",
    "    random_state=random.randrange(10000)\n",
    ")\n",
    "\n",
    "\n",
    "# iterar sobre as diferentes formas de separar os dados em treino e teste\n",
    "# como n_splits=1 acima, apenas é retornada uma\n",
    "# os dados são separados uma única vez em treino e teste\n",
    "# em cada iteração, os dados de treino vão ser separados em treino e validação\n",
    "for train_temp_i, test_i in kf.split(inputs, labels):   \n",
    "    \n",
    "    # split retorna os indices a usar, aqui obtemos os dados que os indices referem\n",
    "    train_temp_inputs = inputs[train_temp_i]\n",
    "    train_temp_labels = labels[train_temp_i]\n",
    "    test_inputs       = inputs[test_i]\n",
    "    test_labels       = labels[test_i]\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # usamos o tempo atual para por os dados de cada run num directório diferente\n",
    "        curr_time = time()\n",
    "        \n",
    "        # para escrever estatísticas em ficheiros ao longo do processo de treino\n",
    "        train_writer = make_file_writer(\"train\", curr_time, sess)\n",
    "        valid_writer = make_file_writer(\"valid\", curr_time, sess)\n",
    "        test_writer  = make_file_writer(\"test\" , curr_time, sess)\n",
    "        \n",
    "        sess.run(init)\n",
    "        for i in range(num_epochs):\n",
    "            # em cada iteração, separar os dados de treino nos conjuntos de validação e de treino\n",
    "            # mantendo proporção de indíviduos de cada classe\n",
    "            kf2 = skl.model_selection.StratifiedShuffleSplit(n_splits=1, test_size=validation_size, random_state=seeds[i])\n",
    "            \n",
    "            for train_i, validation_i in kf2.split(train_temp_inputs, train_temp_labels):\n",
    "                # aceder aos indices retornados\n",
    "                train_inputs      = train_temp_inputs[train_i]  \n",
    "                train_labels      = train_temp_labels[train_i]\n",
    "                validation_inputs = train_temp_inputs[validation_i]\n",
    "                validation_labels = train_temp_labels[validation_i]\n",
    "                \n",
    "                # treino da rede\n",
    "                _, stats = sess.run(\n",
    "                    [optimizer, merged],\n",
    "                    feed_dict={X: train_inputs, y: train_labels}\n",
    "                )\n",
    "                \n",
    "                # escrever as estatísticas de treino para o passo i\n",
    "                train_writer.add_summary(\n",
    "                    summary=stats,\n",
    "                    global_step=i\n",
    "                )        \n",
    "                \n",
    "                # validação (como não é passada a operação de minimização, a rede não é treinada)\n",
    "                acc, stats = sess.run(\n",
    "                    [accuracy, merged], \n",
    "                    feed_dict={X: validation_inputs, y: validation_labels}\n",
    "                )\n",
    "                \n",
    "                # escrever as estatísticas de validação para o passo i\n",
    "                valid_writer.add_summary(\n",
    "                    summary=stats,\n",
    "                    global_step=i\n",
    "                )        \n",
    "            \n",
    "            # periodicamente verificar a performance da rede no conjunto de treino\n",
    "            # a rede nunca é treinada com este conjunto\n",
    "            if i % 100 == 0:\n",
    "                # ao não passar a operação de minimização, a rede não é treinada\n",
    "                acc, stats = sess.run(\n",
    "                    [accuracy, merged], \n",
    "                    feed_dict={X: test_inputs, y: test_labels}\n",
    "                )\n",
    "                # escrever as estatísticas de teste para o passo i\n",
    "                test_writer.add_summary(\n",
    "                    summary=stats, \n",
    "                    global_step=i\n",
    "                )         \n",
    "                print(\"Test Accuracy: \", acc, i)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\minimalsample\\\\model.ckpt2-1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_for_metadata =  os.path.join(LOG_DIR,'metadata.tsv')\n",
    "\n",
    "embedding_var = tf.Variable(inputs, name=\"attributes\")\n",
    "summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "\n",
    "config = tf_projector.ProjectorConfig()\n",
    "embedding = config.embeddings.add()\n",
    "embedding.tensor_name = embedding_var.name\n",
    "\n",
    "# escrever metadados num ficheiro .tsv (tab separated values)\n",
    "with open(path_for_metadata,'w') as f:\n",
    "    sep = \"\\t\"\n",
    "    f.write(\"Index\")\n",
    "    for field in data.dtype.names:\n",
    "        f.write(sep + field)\n",
    "    f.write(sep + \"Labels\\n\")\n",
    "    for index in range(len(inputs)):\n",
    "        f.write(\"{}\".format(index))\n",
    "        for field_i, field in enumerate(data.dtype.names):\n",
    "            f.write(\"{}{}\".format(sep, data[field][index] + random.uniform(0, 0.0001)))\n",
    "            \n",
    "        f.write(\"{}{}\\n\".format(sep, 'ckd' if labels[index][0] == 1 else 'notckd'))\n",
    "        \n",
    "embedding.metadata_path = path_for_metadata #'metadata.tsv'\n",
    "\n",
    "\n",
    "# visualizar embeddings\n",
    "tf_projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, os.path.join(LOG_DIR, \"model.ckpt2\"), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.\\\\logs\\\\1'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(\".\", \"logs\", \"{}\").format(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
